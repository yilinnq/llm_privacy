import os
import json
import requests
import os
import regex as re
import argparse
import pandas as pd
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
from urllib.parse import quote

dotenv_path = os.path.join(os.path.dirname(__file__), '..', '..', '.env')
load_dotenv(dotenv_path=dotenv_path)

# Only create the parser when running as a script, not when imported
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate Q&A response.")
    parser.add_argument('--company_name', type=str, required=True, help="The name of the company")
    parser.add_argument('--question', type=str, help="The question to ask about the privacy policy")
    args = parser.parse_args()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_API_URL = (
    f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"
)

BASE_URL = 'https://transparencydb.dev.berkmancenter.org/company/'

# embedding model
embedder = SentenceTransformer('all-MiniLM-L6-v2')

def load_policy_link(policy_name):
    """
    Loads the policy link from the privacy_db.csv file.
    Falls back to policy_links JSON files if needed.
    """
    # First try to get the link from the CSV
    csv_paths = [
        os.path.join("src", "summary", "privacy_db.csv"),  # If running from root
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "summary", "privacy_db.csv")  # Absolute path
    ]
    
    for csv_path in csv_paths:
        if os.path.exists(csv_path):
            try:
                df = pd.read_csv(csv_path)
                # Try matching with case-insensitive comparison and handling spaces
                match = df[df["Platform"].str.lower() == policy_name.lower().replace("_", " ")]
                
                if not match.empty:
                    txt_url = match.iloc[0]["Privacy Policy Txt"]
                    print(f"‚úÖ Policy file link found in CSV: {txt_url}")
                    return txt_url
            except Exception as e:
                print(f"Error loading from CSV {csv_path}: {str(e)}")
    
    # If we couldn't find it in the CSV, try the JSON files as fallback
    possible_paths = [
        os.path.join("../data_processing/policy_links", f"{policy_name}.json"),  # If running from qa dir
        os.path.join("src/data_processing/policy_links", f"{policy_name}.json"),  # If running from root
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "../data_processing/policy_links", f"{policy_name}.json")  # Absolute path
    ]
    
    for json_file_path in possible_paths:
        if os.path.exists(json_file_path):
            try:
                with open(json_file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                if isinstance(data, str):
                    txt_href = data
                    print(f"‚úÖ Policy file link found in JSON: {txt_href}")
                    return txt_href
                else:
                    raise ValueError(f"Invalid format in {json_file_path}")
            except Exception as e:
                print(f"Error loading {json_file_path}: {str(e)}")
    
    raise FileNotFoundError(f"Could not find policy file for {policy_name}. Make sure the platform exists in privacy_db.csv or has a corresponding JSON file.")

def load_document(txt_href):
    """
    Downloads the document using txt_href link.
    """
    response = requests.get(txt_href)
    if response.status_code != 200:
        raise Exception(f"Could not load the document from {txt_href}")
    print("‚úÖ Policy loaded")
    return response.text

def chunk_text(text, max_chunk_size=500):
    """
    Splits the text into chunks of approximately max_chunk_size words.
    Adjust the splitting logic as needed (e.g., by paragraph).
    """
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_chunk_size):
        chunk = " ".join(words[i:i+max_chunk_size])
        chunks.append(chunk)
    print("‚úÖ Chuncking complete")
    return chunks

def build_index(chunks):
    """
    Embeds the text chunks and builds a FAISS index for similarity search.
    """
    embeddings = embedder.encode(chunks, convert_to_tensor=False)
    embeddings = np.array(embeddings).astype("float32")
    d = embeddings.shape[1]
    index = faiss.IndexFlatIP(d)
    faiss.normalize_L2(embeddings)
    index.add(embeddings)
    print("‚úÖ FAISS embedding complete")
    return index, embeddings

def retrieve_relevant_chunks(question, chunks, index, chunk_embeddings, top_k=3):
    """
    Embeds the question and retrieves the top_k most similar text chunks.
    """
    question_embedding = embedder.encode([question], convert_to_tensor=False)
    question_embedding = np.array(question_embedding).astype("float32")
    faiss.normalize_L2(question_embedding)
    D, I = index.search(question_embedding, top_k)
    retrieved = [chunks[i] for i in I[0]]
    print("‚úÖ Reterieved context")
    return retrieved

def generate_answer(question, context_chunks):
    """
    Generates an answer by calling the Gemini API using the gemini-2.0-flash-lite model.
    The API is called via a REST POST request with a payload that mirrors the provided curl example.
    """
    context = " ".join(context_chunks)
    input_text = f"question: {question} context: {context}"
    
    # Construct payload per the Gemini API documentation.
    payload = {
        "system_instruction": {
            "parts": [
                {
                "text": "You are a privacy policy expert. You will answer user's question accurately given the context provided, and you will include the reference text content you used to generate the answer by marking it as 'Reference Used:'."
                }
            ]
        },
        "contents": [
            {
                "parts": [
                    {
                        "text": input_text
                    }
                ]
            }
        ]
    }
    
    headers = {
        "Content-Type": "application/json"
    }
    
    response = requests.post(GEMINI_API_URL, headers=headers, json=payload)
    if response.status_code != 200:
        raise Exception(f"Gemini API call failed with status code {response.status_code}: {response.text}")
    
    result = response.json()
    # print("üêõ Debug Gemini API response:", result)
    
    candidate = result.get("candidates", [{}])[0]
    answer = candidate.get("content", {}).get("parts", [{}])[0].get("text")
    
    if not answer:
        raise Exception("No answer found in the Gemini API response.")
    return answer

# def extract_reference_snippet(answer_text):
#     """
#     Extracts the reference snippet from the LLM's answer based on a "Reference Used:" marker.
#     Returns the snippet text if found, otherwise returns None.
#     """
#     match = re.search(r"Reference Used:\s*(.*)", answer_text, re.DOTALL)
#     if match:
#         # Get the first line of the reference or a trimmed version
#         snippet = match.group(1).strip().splitlines()[0]
#         return snippet
#     return None

# def generate_highlight_link(base_url, chunk_text):
#     """
#     Generates a text-fragment highlight link using a snippet from the context LLM used to generate answers.
#     """
#     sentences = re.split(r'(?<=[.!?])\s+', chunk_text)
    
#     # Choose the longest sentence if available.
#     candidate_sentence = max(sentences, key=lambda s: len(s)) if sentences else chunk_text
    
#     # Optionally trim the candidate to a maximum number of characters (e.g., 200) to avoid overly long fragments.
#     max_length = 200
#     if len(candidate_sentence) > max_length:
#         candidate_sentence = candidate_sentence[:max_length]
    
#     encoded_snippet = quote(candidate_sentence)
#     highlight_link = f"{base_url}#:~:text={encoded_snippet}"
#     return highlight_link

def main(company_name, user_question):
    txt_href = load_policy_link(company_name)
    document = load_document(txt_href)
    chunks = chunk_text(document)
    index, chunk_embeddings = build_index(chunks)
    relevant_chunks = retrieve_relevant_chunks(user_question, chunks, index, chunk_embeddings)
    answer = generate_answer(user_question, relevant_chunks)
    return answer, txt_href

    # reference_snippet = extract_reference_snippet(answer)
    # if reference_snippet:
    #     highlight_link = generate_highlight_link(txt_href, reference_snippet)
    # else:
    #     highlight_link = generate_highlight_link(txt_href, relevant_chunks[0])
    # return answer, highlight_link

if __name__ == "__main__":
    if args.question:
        user_question = args.question
    else:
        user_question = input(f"Please enter your question about {args.company_name}'s privacy policy: ")
    
    answer, txt_href = main(args.company_name, user_question)
    # answer, highlight_link = main(args.company_name, user_question)
    print("Answer:", answer)
    print("Reference URL:", txt_href)
    # print("\nHighlight Link:", highlight_link)
